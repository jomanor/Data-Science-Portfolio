{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqfUz-Q1fZJK"
      },
      "source": [
        "\n",
        "### **Algoritmos de clasificación con Pyspark**\n",
        "\n",
        "**Por: José María Manzano Ortega**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGeuySu9X9xU"
      },
      "source": [
        "##**1. Instalación Apache Spark en Google Colab.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aosumELxpCqc"
      },
      "source": [
        "En este cuaderno (notebook) se van a implementar un par de algoritmos de clasificación sobre el dataset de 10k instancias de **SUSY**, los comandos para la instalación y ejecución del entorno Spark siguen como se muestran a continuación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wf76hHtnAgS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c36dff89-da84-4758-8652-251317f802ef"
      },
      "source": [
        "#Primero instalamos Apache Spark con Hadoop\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.3-bin-hadoop3.tgz\n",
        "\n",
        "#Instalamos los paquetes de Python para trabajar con Spark\n",
        "!pip install findspark #Instalamos FindSpark\n",
        "!pip install pyspark   #Instalamos Spark\n",
        "\n",
        "#Indicamos a PySpark donde está Spark\n",
        "import findspark\n",
        "findspark.init(\"spark-3.5.3-bin-hadoop3\")#SPARK_HOME\n",
        "\n",
        "#Inicializamos las variables de entorno\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.3-bin-hadoop3\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNoxOlfPKtLu"
      },
      "source": [
        "##**2. Incoporar el SparkPackage de CatBoost en la sesión.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para poder usar un paquete de SparkPackages se lo tenemos que indicar al cluster. Para que se lo descargue y lo pueda ejecutar.\n",
        "\n",
        "En este ejemplo vamos a usar el paquete CatBoost para Spark que es de los pocos que tienen tanto soporte en Python como en Scala. Para ello lo haremos al crear la sesión, usando el método config:\n",
        "\n",
        "```\n",
        "config(\"spark.jars.packages\", \"ai.catboost:catboost-spark_3.5_2.12:1.2.3\")\n",
        "```\n",
        "\n",
        "Con este le indicamos que vamos a usar una bibliotecar externa en un jar de SparkPackages y dicha biblioteca es `ai.catboost:catboost-spark_3.5_2.12:1.2.3`.\n"
      ],
      "metadata": {
        "id": "k5nRYk-ew4Xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Creamos una sesión de Spark para poder trabajar\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Ejemplo de CatBoost en PySpark\") \\\n",
        "    .config(\"spark.jars.packages\", \"ai.catboost:catboost-spark_3.5_2.12:1.2.3\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "3YTce6JQGd53"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFTWhaypw8em"
      },
      "source": [
        "##**3. Preparamos los datos.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZp4x101r72b"
      },
      "source": [
        "Lo primero que vamos a hacer es descargarnos los datos con los que vamos a trabajar y guardarlos en dos DataFrame: uno de entrenamiento (dfTra) y otro de test (dfTst)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fe982de-83cf-4053-ea07-1e13b880b15e",
        "id": "D3eRlSjTzdMd"
      },
      "source": [
        "#Nos descargamos los ficheros de datos en Google Colab\n",
        "!wget -nv --no-check-certificate 'https://docs.google.com/uc?export=download&id=1HOrM49tCLA_NqHyD_ps_cv483FPN7aWo' -O susy-10k-tra.csv\n",
        "!wget -nv --no-check-certificate 'https://docs.google.com/uc?export=download&id=1HT80d5cwU7HMi2XK8CNgxgHvxRZEZB_d' -O susy-10k-tst.csv\n",
        "\n",
        "#Leemos los conjuntos de entrenamiento y test\n",
        "dfTra = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"inferSchema\",True) \\\n",
        "    .option(\"header\", True) \\\n",
        "    .load(\"susy-10k-tra.csv\")\n",
        "dfTst = spark.read.csv('susy-10k-tst.csv', inferSchema=True, header=True)\n",
        "\n",
        "dfTra.show(5)\n",
        "dfTst.show(5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-26 15:10:14 URL:https://drive.usercontent.google.com/download?id=1HOrM49tCLA_NqHyD_ps_cv483FPN7aWo&export=download [3463157/3463157] -> \"susy-10k-tra.csv\" [1]\n",
            "2024-09-26 15:10:17 URL:https://drive.usercontent.google.com/download?id=1HT80d5cwU7HMi2XK8CNgxgHvxRZEZB_d&export=download [3469204/3469204] -> \"susy-10k-tst.csv\" [1]\n",
            "+------------------+-------------------+--------------------+-------------------+--------------------+-------------------+-------------------+--------------------+-------------------+--------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+-----+\n",
            "|               uno|                dos|                tres|             cuatro|               cinco|               seis|              siete|                ocho|              nueve|                diez|              once|              doce|             trece|            catorce|            quince|         dieciseis|        diecisiete|          dieciocho|clase|\n",
            "+------------------+-------------------+--------------------+-------------------+--------------------+-------------------+-------------------+--------------------+-------------------+--------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+-----+\n",
            "|0.6433419585227966| 1.3615427017211914|  0.6828370690345764|0.48453789949417114|-0.32587578892707825|-0.8524075746536255|  1.145269513130188|  0.8196632862091064|0.40248867869377136|   1.325475811958313|0.7232717275619507|0.6548330783843994|0.8034266233444214|0.37070924043655396|0.7308408617973328|0.5756281018257141|1.4113106727600098|0.19226300716400146|  1.0|\n",
            "|1.2374616861343384|-0.4839438199996948|  1.3800559043884277| 1.6685774326324463|  0.4818389117717743|-0.6302737593650818| 0.6783822178840637|-0.43130290508270264|0.34755226969718933|-0.01465716026723...|1.3650861978530884| 1.040429711341858|0.6763473749160767| 0.6026328206062317|  1.36842942237854|0.7144116759300232|0.9274933934211731|0.16320399940013885|  1.0|\n",
            "|1.3433129787445068|  0.783230721950531| -0.7952934503555298|  1.475182294845581|  0.6200259327888489|0.43646135926246643| 0.8689783811569214|   1.131447672843933| 0.3842912018299103|-0.49663621187210083|1.1656821966171265|1.3288121223449707|1.0115764141082764| 1.8998759984970093|1.1543011665344238| 1.430264949798584|0.6719890236854553|0.28056100010871887|  0.0|\n",
            "|0.6531051993370056| 0.9232335686683655|-0.31942081451416016|  0.519090473651886|-0.06831558048725128|-1.6212221384048462|0.38024044036865234|  0.8473420143127441|0.30507954955101013|-0.00403195200487...|0.5825600624084473|0.5741020441055298|0.8745092153549194| 0.8703949451446533| 0.573835015296936|0.6802302598953247|0.6731339693069458|0.19332100450992584|  0.0|\n",
            "|1.5516648292541504|-0.4539290964603424|  1.4226588010787964|  2.352872848510742|  0.7626277804374695|0.03150284290313721| 1.0506685972213745| -0.7840972542762756|  1.571985125541687| -0.7112254500389099|1.9744004011154175|1.5981488227844238|0.7182844281196594| 2.3227782249450684|1.9981735944747925|1.7646944522857666|   1.2172771692276|0.06346789747476578|  1.0|\n",
            "+------------------+-------------------+--------------------+-------------------+--------------------+-------------------+-------------------+--------------------+-------------------+--------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------------------+-------------------+--------------------+-------------------+--------------------+-------------------+------------------+-------------------+------------------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+--------------------+-----+\n",
            "|                uno|                dos|                tres|             cuatro|               cinco|               seis|             siete|               ocho|             nueve|                diez|              once|              doce|             trece|           catorce|            quince|         dieciseis|        diecisiete|           dieciocho|clase|\n",
            "+-------------------+-------------------+--------------------+-------------------+--------------------+-------------------+------------------+-------------------+------------------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+--------------------+-----+\n",
            "| 1.6795326471328735|-1.3285973072052002|   1.721287727355957| 0.8961411714553833|  0.7851014137268066| -0.300307035446167|2.4243409633636475|0.24690665304660797| 3.021253824234009| -2.0464718341827393|1.9866414070129395|2.2912914752960205|1.0234774351119995|2.0351436138153076| 2.139514207839966| 2.923895835876465|1.4491009712219238|  0.2602680027484894|  1.0|\n",
            "| 0.8576836585998535|0.10346849262714386|-0.15959890186786652|0.49158814549446106|-0.46780043840408325| 1.6336811780929565|1.2118911743164062| 1.1086699962615967| 1.492547631263733| -0.5306073427200317|0.6292113065719604|1.0887541770935059|1.5354979038238525|0.5368104577064514|0.7166863679885864|0.8678085207939148|1.2548242807388306|  0.5520399808883667|  1.0|\n",
            "|  0.679413378238678| 0.6467429399490356| -0.7623209953308105| 1.0956929922103882| 0.04633910581469536| 1.1650936603546143|1.0221043825149536| -1.074870228767395|0.8398934602737427|  1.1044260263442993|0.7407406568527222|0.8633846044540405|1.0343234539031982|               0.0| 0.770002007484436|0.4708144962787628|1.4442930221557617| 0.24737399816513062|  1.0|\n",
            "| 0.7668826580047607| 0.4359167516231537|  -1.404170036315918| 0.7847794890403748| -0.8897899389266968|-0.7484598755836487|0.8655236959457397| 0.8442953824996948|1.2992560863494873| -0.4516412317752838| 0.851235032081604| 1.084952712059021| 1.131034255027771|2.2521724700927734|0.8331539630889893|1.6848599910736084|0.7220147848129272| 0.09403660148382187|  1.0|\n",
            "|0.48874709010124207| 0.4925791323184967|  1.6006025075912476| 0.6941078305244446| -1.8233418464660645| 0.4462366998195648|0.4422283172607422|-0.9721801280975342|0.6638392210006714|-0.04252281785011...|1.0111192464828491| 0.596332311630249|0.5233632922172546|1.0546964406967163|1.0142439603805542|0.7886103987693787|0.8621771931648254|0.012908900156617165|  0.0|\n",
            "+-------------------+-------------------+--------------------+-------------------+--------------------+-------------------+------------------+-------------------+------------------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv6vhHquzlB_"
      },
      "source": [
        "  Ahora preparamos los datos para poder trabajar con ellos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-kGFhtwznCn"
      },
      "source": [
        "#Preprocesamos los datos para usarlos en un algoritmo de aprendizaje supervisado\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "#Convertimos la variable clase en entera\n",
        "dfTra=dfTra.withColumn(\"clase\",dfTra.clase.cast(\"Integer\"))\n",
        "dfTst=dfTst.withColumn(\"clase\",dfTst.clase.cast(\"Integer\"))\n",
        "\n",
        "#Cambiamos el nombre de la columna donde está la variable de salida\n",
        "dfTra=dfTra.withColumnRenamed(\"clase\",\"label\")\n",
        "dfTst=dfTst.withColumnRenamed(\"clase\",\"label\")\n",
        "\n",
        "#Unimos los atributos con VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=dfTra.columns[:-1],outputCol=\"features\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8iup3ks5LXl"
      },
      "source": [
        "##**4. Ejemplo con `Decision Tree`.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos ahora un clasificador de los que vienen implementados en Apache Spark:"
      ],
      "metadata": {
        "id": "tvBufjQe0hE7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKZQUkv3OZaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b44eceeb-ddb1-462a-9799-ab6d6ef040be"
      },
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "#Definimos el árbol de decisión (DT)\n",
        "dt = DecisionTreeClassifier(maxDepth=4)\n",
        "\n",
        "#Creamos una tubería, con el preprocesamiento y el DT\n",
        "tuberia=Pipeline().setStages( [assembler, dt] )\n",
        "\n",
        "#Construimos el modelo con los datos de entrenamiento\n",
        "modeloDT = tuberia.fit(dfTra)\n",
        "\n",
        "#Hacemos predicciones con el modelo aprendido (preprocesamiento + DT)\n",
        "predicciones = modeloDT.transform(dfTst)\n",
        "\n",
        "#Ya podemos mostrar la bondad del modelo\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluador = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
        "print('Accuracy Decision Tree:', evaluador.evaluate(predicciones))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Decision Tree: 0.7763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sRl3QpS5ZA9"
      },
      "source": [
        "##**5. Ejemplo con `CatBoost`.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "De entre los algoritmos más populares que se basan en árboles de potenciación del gradiente encontramos CatBoost. Que está disponible en SparkPackages y se puede usar desde Python y Scala.\n"
      ],
      "metadata": {
        "id": "0usqlnx52g4W"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "028f7930-6a3b-474e-81f4-07bdab9281d4",
        "id": "xW30Pg2vsuK-"
      },
      "source": [
        "import catboost_spark\n",
        "\n",
        "catb = catboost_spark.CatBoostClassifier(iterations=10)\n",
        "\n",
        "#Creamos una tubería, con el preprocesamiento y CatBoost\n",
        "tuberia=Pipeline( stages=[assembler, catb] )\n",
        "\n",
        "#Construimos el modelo con los datos de entrenamiento\n",
        "modelolcatB = tuberia.fit(dfTra)\n",
        "\n",
        "#Hacemos predicciones con el modelo aprendido (preprocesamiento + CatBoost)\n",
        "predicciones = modelolcatB.transform(dfTst)\n",
        "\n",
        "#Ya podemos mostrar la bondad del modelo\n",
        "print('Accuracy CatBoost:', evaluador.evaluate(predicciones))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy CatBoost: 0.8037\n"
          ]
        }
      ]
    }
  ]
}
